mcp_llm_project/
â”œâ”€â”€ .env                        # Secure environment variables (e.g. API keys)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Optional: containerized setup
â”‚
â”œâ”€â”€ llm_client.py               # LLM API handler with retry & logging
â”œâ”€â”€ context_builder.py          # Creates enriched task prompts
â”œâ”€â”€ main.py                     # MCP client orchestrating tools and LLM context
â”‚
â”œâ”€â”€ tests/                      # Unit tests for modular components
â”‚   â””â”€â”€ test_prompt_builder.py
â”‚
â””â”€â”€ llm_prompt_playground.ipynb # Notebook for tuning LLM prompts

# MCP + LLM Context Enrichment Project

This project integrates [FastMCP](https://github.com/fastmcp/fastmcp) with a Large Language Model (LLM) like OpenAI to enrich task requests with natural language context. It demonstrates how structured prompts and semantic understanding can enhance computation workflows.

---

## ðŸ§  Features

- MCP client that connects to a tool-based computation server
- Integration with LLMs (OpenAI by default) for context-aware enrichment
- Modular prompt generation and API handling
- Retry logic and logging for resiliency and observability
- `.env` secret management
- Jupyter notebook for prompt tuning and model comparison

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/your-username/mcp-llm-project
cd mcp_llm_project
pip install -r requirements.txt
